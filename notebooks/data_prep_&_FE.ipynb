{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "In this notebook we will do some data preparation and feature engineering process before the modelling phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "using relative paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current file name: data_prep_&_FE.ipynb\n",
      "\n",
      "Current absolute path: c:\\Users\\jhona\\Dropbox\\ASPECTOS MAESTRIA\\Retos_maestria\\Reto_bancow\\Analitica_1\\notebooks\n",
      "\n",
      "BASE_DIR: c:\\Users\\jhona\\Dropbox\\ASPECTOS MAESTRIA\\Retos_maestria\\Reto_bancow\\Analitica_1\n",
      "DATA_DIR: c:\\Users\\jhona\\Dropbox\\ASPECTOS MAESTRIA\\Retos_maestria\\Reto_bancow\\Analitica_1\\Data\n",
      "OUTPUT_DIR: c:\\Users\\jhona\\Dropbox\\ASPECTOS MAESTRIA\\Retos_maestria\\Reto_bancow\\Analitica_1\\Data\\output_data\n"
     ]
    }
   ],
   "source": [
    "filename = \"data_prep_&_FE.ipynb\"  # Current file name\n",
    "print(f\"Current file name: {filename}\\n\")\n",
    "print(f\"Current absolute path: {os.getcwd()}\\n\")\n",
    "\n",
    "# Specify the paths, relative to the current file\n",
    "ACTUAL_DIR = os.path.dirname(os.path.abspath(filename))\n",
    "BASE_DIR = os.path.dirname(ACTUAL_DIR)\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"Data\")\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, \"output_data\")\n",
    "\n",
    "print(f\"BASE_DIR: {BASE_DIR}\")\n",
    "print(f\"DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(os.path.join(OUTPUT_DIR, \"hallazgos_clean.xlsx\")).iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the data shape, the NaN values in the data and the column formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2916 entries, 0 to 2915\n",
      "Data columns (total 24 columns):\n",
      " #   Column                   Non-Null Count  Dtype         \n",
      "---  ------                   --------------  -----         \n",
      " 0   cliente                  2915 non-null   object        \n",
      " 1   analista                 2916 non-null   object        \n",
      " 2   tipo_credito             2916 non-null   object        \n",
      " 3   actividad                2916 non-null   object        \n",
      " 4   actividad_v2             2916 non-null   object        \n",
      " 5   monto                    2916 non-null   int64         \n",
      " 6   cuota                    2916 non-null   int64         \n",
      " 7   plazo                    2916 non-null   int64         \n",
      " 8   oficina                  2916 non-null   object        \n",
      " 9   zona                     2916 non-null   int64         \n",
      " 10  regional                 2916 non-null   int64         \n",
      " 11  fecha_desembolso         2913 non-null   datetime64[ns]\n",
      " 12  visita_analista_credito  2127 non-null   datetime64[ns]\n",
      " 13  calificacion_cartera     2916 non-null   object        \n",
      " 14  relaciones_laborales     2916 non-null   object        \n",
      " 15  estado                   2916 non-null   object        \n",
      " 16  year                     2916 non-null   int64         \n",
      " 17  tipo                     2916 non-null   object        \n",
      " 18  categoria                2916 non-null   object        \n",
      " 19  hallazgo                 2916 non-null   object        \n",
      " 20  tipo_hallazgo            2916 non-null   object        \n",
      " 21  riesgo                   2916 non-null   object        \n",
      " 22  riesgo_int               2916 non-null   int64         \n",
      " 23  riesgo_bin               2916 non-null   int64         \n",
      "dtypes: datetime64[ns](2), int64(8), object(14)\n",
      "memory usage: 546.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some columns with NaN values, **we will drop the observations with NaN values in the date of disbursement** (Only 3).  \n",
    "The NaN values in the columns related to the visit of the microcredit analyst is something expected due to the context. We will keep this observations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=\"fecha_desembolso\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "In this section we will complete the cleaning and structuring of the dataset, in order to avoid inconveniences in the modeling stage.\n",
    "\n",
    "1. First, lets remove all the variables that can only be measured after an audit. It is important to drop this columns because they can cause problems such as **data leakage**.  \n",
    "2. We will remove all columns that do not give us valuable information, such as the customer's name. \n",
    "3. We will cast the format of the zone and region variables to object type, since they are nominal variables, not ordinals.\n",
    "4. As a finding of the exploratory data analysis, we will perform a filtering of outliers of the loan term variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- drop columns to avoid data leakage\n",
    "df.drop([\"hallazgo\",\"calificacion_cartera\",\"relaciones_laborales\", \"tipo\",\"categoria\",\"tipo_hallazgo\"],axis=1,inplace=True)\n",
    "\n",
    "# 2- drop columns withut valuable information\n",
    "df.drop([\"year\",\"cliente\",\"actividad\", \"riesgo\"],axis=1,inplace=True)\n",
    "\n",
    "# 3 Cast the \"zona\" and \"regional\" to object variables\n",
    "df[[\"zona\",\"regional\"]] = df[[\"zona\",\"regional\"]].astype(object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers upper limit: 102.0\n",
      "Outliers lower limit: 0\n",
      "\n",
      "New shape of dataset: (2910, 14)\n"
     ]
    }
   ],
   "source": [
    "# 4 identify the outliers of loan amount and drop it\n",
    "\n",
    "# Q1, Q3 & IQR\n",
    "Q1 = np.percentile(df.plazo, 25)  # Q1 (25%)\n",
    "Q3 = np.percentile(df.plazo, 75)  # Q3 (75%)\n",
    "IQR = Q3 - Q1  # IQR\n",
    "\n",
    "# Limits for outliers\n",
    "lower_limit = Q1 - 3 * IQR \n",
    "lower_limit = 0 if lower_limit<0 else lower_limit\n",
    "upper_limit = Q3 + 3 * IQR\n",
    "\n",
    "print(f'Outliers upper limit: {upper_limit}')\n",
    "print(f'Outliers lower limit: {lower_limit}')\n",
    "\n",
    "# Identify outlier\n",
    "outliers = df.plazo[(df.plazo < lower_limit) | (df.plazo > upper_limit)]\n",
    "upper_outliers = df[(df.plazo > upper_limit)]\n",
    "\n",
    "df = df[df[\"plazo\"]<upper_limit]\n",
    "print(f'\\nNew shape of dataset: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2910 entries, 0 to 2915\n",
      "Data columns (total 14 columns):\n",
      " #   Column                   Non-Null Count  Dtype         \n",
      "---  ------                   --------------  -----         \n",
      " 0   analista                 2910 non-null   object        \n",
      " 1   tipo_credito             2910 non-null   object        \n",
      " 2   actividad_v2             2910 non-null   object        \n",
      " 3   monto                    2910 non-null   int64         \n",
      " 4   cuota                    2910 non-null   int64         \n",
      " 5   plazo                    2910 non-null   int64         \n",
      " 6   oficina                  2910 non-null   object        \n",
      " 7   zona                     2910 non-null   object        \n",
      " 8   regional                 2910 non-null   object        \n",
      " 9   fecha_desembolso         2910 non-null   datetime64[ns]\n",
      " 10  visita_analista_credito  2121 non-null   datetime64[ns]\n",
      " 11  estado                   2910 non-null   object        \n",
      " 12  riesgo_int               2910 non-null   int64         \n",
      " 13  riesgo_bin               2910 non-null   int64         \n",
      "dtypes: datetime64[ns](2), int64(5), object(7)\n",
      "memory usage: 341.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "--> In this section, we aim to transform and create new features that improve the performance of the model. Moreover, we will do:  \n",
    "1. Creation of new variables from the existing ones.\n",
    "2. Extraction of information from time series or spatial data. \n",
    "3. Division of the dataset based on loan amounts.\n",
    "4. Coding of categorical variables (one-hot encoding).  \n",
    "5. Creation of pipelines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation and modification of features\n",
    "\n",
    "--> We proceed to create new variables in search of better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New dates columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2910 entries, 0 to 2915\n",
      "Data columns (total 2 columns):\n",
      " #   Column                   Non-Null Count  Dtype         \n",
      "---  ------                   --------------  -----         \n",
      " 0   fecha_desembolso         2910 non-null   datetime64[ns]\n",
      " 1   visita_analista_credito  2121 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](2)\n",
      "memory usage: 68.2 KB\n"
     ]
    }
   ],
   "source": [
    "df[[\"fecha_desembolso\",\"visita_analista_credito\"]].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we are gonna create some temporal variables in order to capture patterns\n",
    "\n",
    "# Day of the week of the disbursement\n",
    "df[\"dia_semana_desembolso\"] = df[\"fecha_desembolso\"].dt.weekday\n",
    "\n",
    "# Day of the month of the disbursement\n",
    "df[\"dia_mes_desembolso\"] = df[\"fecha_desembolso\"].dt.day\n",
    "\n",
    "# Month of the disbursement\n",
    "df[\"mes_desembolso\"] = df[\"fecha_desembolso\"].dt.month\n",
    "\n",
    "# Difference in days between the analyst's visit and the disbursement (NaN if any date is missing)\n",
    "df[\"dias_desde_visita_a_desembolso\"] = (df[\"fecha_desembolso\"] - df[\"visita_analista_credito\"]).dt.days\n",
    "\n",
    "# Is the disbursement on a weekend? (1 = Yes, 0 = No)\n",
    "df[\"desembolso_fin_de_semana\"] = df[\"fecha_desembolso\"].dt.weekday.isin([5, 6]).astype(int)\n",
    "\n",
    "# Is the disbursement at the end of the month? (1 = Yes, 0 = No)\n",
    "df[\"desembolso_fin_de_mes\"] = df[\"fecha_desembolso\"].dt.day.isin([28, 29, 30, 31]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, lets drop the datetime variables\n",
    "\n",
    "df = df.drop([\"fecha_desembolso\",\"visita_analista_credito\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dia_semana_desembolso</th>\n",
       "      <th>dia_mes_desembolso</th>\n",
       "      <th>mes_desembolso</th>\n",
       "      <th>dias_desde_visita_a_desembolso</th>\n",
       "      <th>desembolso_fin_de_semana</th>\n",
       "      <th>desembolso_fin_de_mes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2910.000000</td>\n",
       "      <td>2910.000000</td>\n",
       "      <td>2910.000000</td>\n",
       "      <td>2121.000000</td>\n",
       "      <td>2910.000000</td>\n",
       "      <td>2910.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.612027</td>\n",
       "      <td>20.201031</td>\n",
       "      <td>6.656701</td>\n",
       "      <td>1.742103</td>\n",
       "      <td>0.124399</td>\n",
       "      <td>0.280069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.549124</td>\n",
       "      <td>8.343341</td>\n",
       "      <td>2.905646</td>\n",
       "      <td>28.160908</td>\n",
       "      <td>0.330092</td>\n",
       "      <td>0.449110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-366.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dia_semana_desembolso  dia_mes_desembolso  mes_desembolso  \\\n",
       "count            2910.000000         2910.000000     2910.000000   \n",
       "mean                2.612027           20.201031        6.656701   \n",
       "std                 1.549124            8.343341        2.905646   \n",
       "min                 0.000000            1.000000        1.000000   \n",
       "25%                 1.000000           13.000000        5.000000   \n",
       "50%                 3.000000           21.000000        7.000000   \n",
       "75%                 4.000000           28.000000        9.000000   \n",
       "max                 6.000000           31.000000       12.000000   \n",
       "\n",
       "       dias_desde_visita_a_desembolso  desembolso_fin_de_semana  \\\n",
       "count                     2121.000000               2910.000000   \n",
       "mean                         1.742103                  0.124399   \n",
       "std                         28.160908                  0.330092   \n",
       "min                       -366.000000                  0.000000   \n",
       "25%                          1.000000                  0.000000   \n",
       "50%                          2.000000                  0.000000   \n",
       "75%                          5.000000                  0.000000   \n",
       "max                        381.000000                  1.000000   \n",
       "\n",
       "       desembolso_fin_de_mes  \n",
       "count            2910.000000  \n",
       "mean                0.280069  \n",
       "std                 0.449110  \n",
       "min                 0.000000  \n",
       "25%                 0.000000  \n",
       "50%                 0.000000  \n",
       "75%                 1.000000  \n",
       "max                 1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\n",
    "    [\n",
    "        \"dia_semana_desembolso\",\n",
    "        \"dia_mes_desembolso\",\n",
    "        \"mes_desembolso\",\n",
    "        \"dias_desde_visita_a_desembolso\",\n",
    "        \"desembolso_fin_de_semana\",\n",
    "        \"desembolso_fin_de_mes\",\n",
    "    ]\n",
    "].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that the variable \"dias_desde_visita_a_desembolso\" presents high negative and high positive values. These values are not the majority, they may be errors and need further analysis with the bank's team.  \n",
    "\n",
    " --> For this analysis, **we would modify the \"dias_desde_visita_a_desembolso\" column in order to avoid the extreme values**. We will turn NAN some of that values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of audits with the analyst visit after the disburment: 74\n",
      "Number of audits with the analyst visit before the disburment: 1670\n",
      "Number of NaN values for \"dias_desde_visita_a_desembolso\": 789\n",
      "\n",
      "After modification:\n",
      "\n",
      "Number of audits with the analyst visit after the disburment: 23\n",
      "Number of audits with the analyst visit before the disburment: 1632\n",
      "Number of NaN values for \"dias_desde_visita_a_desembolso\": 878\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of audits with the analyst visit after the disburment: {df[df[\"dias_desde_visita_a_desembolso\"] < 0].shape[0]}')\n",
    "print(f'Number of audits with the analyst visit before the disburment: {df[df[\"dias_desde_visita_a_desembolso\"] > 0].shape[0]}')\n",
    "\n",
    "print(f'Number of NaN values for \"dias_desde_visita_a_desembolso\": {df[\"dias_desde_visita_a_desembolso\"].isna().sum()}')\n",
    "\n",
    "# Considering only 21 days (3 weeks) before or after the loan disbursement\n",
    "df[\"dias_desde_visita_a_desembolso\"] = np.where(\n",
    "    df[\"dias_desde_visita_a_desembolso\"] < -21,\n",
    "    np.nan,\n",
    "    np.where(\n",
    "        df[\"dias_desde_visita_a_desembolso\"] > 21,\n",
    "        np.nan,\n",
    "        df[\"dias_desde_visita_a_desembolso\"],\n",
    "    ),\n",
    ")\n",
    "print(\"\\nAfter modification:\\n\")\n",
    "\n",
    "print(f'Number of audits with the analyst visit after the disburment: {df[df[\"dias_desde_visita_a_desembolso\"] < 0].shape[0]}')\n",
    "print(f'Number of audits with the analyst visit before the disburment: {df[df[\"dias_desde_visita_a_desembolso\"] > 0].shape[0]}')\n",
    "print(f'Number of NaN values for \"dias_desde_visita_a_desembolso\": {df[\"dias_desde_visita_a_desembolso\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New location-office columns\n",
    "\n",
    "We will develop a couple of columns taking in consideration the ubication of the offices.  \n",
    "We are taking into account the following information about the bank offices:  \n",
    " https://www.bancow.com.co/wp-content/uploads/2024/06/Horarios-Actuales-Oficinas-Banco-W.pdf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dosquebradas', 'tulua', 'independencia', 'laureano_gómez',\n",
       "       'quibdo', 'puerto_gaitan', 'pasto_las_lunas', 'ipiales',\n",
       "       'la_unión', 'poblado', 'pasto', 'cartagena_del_chaira',\n",
       "       'sincelejo', 'facatativa', 'engativa', 'tulua_calle_25',\n",
       "       'santa_rosa', 'pereira_lagos', 'santa_marta', 'duitama', 'corozal',\n",
       "       'soledad', 'santander_de_quilichao', 'ibague', 'santa_librada',\n",
       "       'chaparral', 'murillo_toro', 'belen', 'alfonso_lópez', 'zipaquira',\n",
       "       'riohacha', 'la_dorada', 'armenia_sur', 'calima', 'fonsecca',\n",
       "       'sabanalarga', 'cereté', 'rionegro', 'san_juan_del_cesar',\n",
       "       'quebradaseca', 'giron', 'itagui', 'soacha', 'villeta',\n",
       "       'barranquilla', 'pradera', 'patio_bonito', 'chinchiná', 'la_plata',\n",
       "       'autopista_norte', 'jamundí', 'kennedy', 'sur', 'palmira',\n",
       "       'maicao', 'el_espinal', 'magangue', 'apartado', 'calle_25',\n",
       "       'fundación', 'buenaventura'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.oficina.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oficina</th>\n",
       "      <th>Departamento</th>\n",
       "      <th>distance_to_dto_capital</th>\n",
       "      <th>is_rural</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>la dorada</td>\n",
       "      <td>Caldas</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>fonsecca</td>\n",
       "      <td>Guajira</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      oficina Departamento  distance_to_dto_capital  is_rural\n",
       "31  la dorada       Caldas                      120         0\n",
       "34   fonsecca      Guajira                       60         1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Dictionary of distances to the capital of the department (aprox)\n",
    "distancias_aprox = {\n",
    "    \"dosquebradas\": 10, \"poblado\": 5, \"tulua\": 100, \"independencia\": 3, \"laureano gómez\": 5,\n",
    "    \"quibdo\": 0, \"puerto gaitan\": 180, \"pasto las lunas\": 0, \"ipiales\": 80, \"la unión\": 120,\n",
    "    \"pasto\": 0, \"cartagena del chaira\": 250, \"sincelejo\": 0, \"facatativa\": 45, \"engativa\": 10,\n",
    "    \"tulua calle 25\": 100, \"santa rosa\": 20, \"pereira lagos\": 5, \"santa marta\": 0, \"duitama\": 50,\n",
    "    \"corozal\": 15, \"soledad\": 5, \"santander de quilichao\": 80, \"ibague\": 0, \"santa librada\": 10,\n",
    "    \"chaparral\": 150, \"murillo toro\": 30, \"belen\": 5, \"alfonso lópez\": 5, \"zipaquira\": 45,\n",
    "    \"riohacha\": 0, \"la dorada\": 120, \"armenia sur\": 5, \"calima\": 90, \"fonsecca\": 60,\n",
    "    \"sabanalarga\": 40, \"cereté\": 20, \"rionegro\": 30, \"san juan del cesar\": 100, \"quebradaseca\": 5,\n",
    "    \"giron\": 10, \"itagui\": 10, \"soacha\": 20, \"villeta\": 80, \"barranquilla\": 0, \"pradera\": 35,\n",
    "    \"patio bonito\": 15, \"chinchiná\": 30, \"la plata\": 180, \"autopista norte\": 5, \"jamundí\": 25,\n",
    "    \"kennedy\": 10, \"sur\": 5, \"palmira\": 20, \"maicao\": 80, \"el espinal\": 30, \"magangue\": 200,\n",
    "    \"apartado\": 310, \"calle 25\": 100, \"fundación\": 70, \"buenaventura\": 120\n",
    "}\n",
    "\n",
    "# List of rural cities according to:\n",
    "ciudades_rurales = {\"puerto gaitan\", \"cartagena del chaira\", \"san juan del cesar\", \"fonsecca\",\n",
    "                    \"sabanalarga\", \"la unión\", \"chaparral\", \"la plata\", \"apartado\", \"fundación\"}\n",
    "\n",
    "# Offices with their department\n",
    "datos_oficinas = [(ciudad.lower(), depto) for ciudad, depto in [\n",
    "    (\"Dosquebradas\", \"Risaralda\"), (\"Poblado\", \"Antioquia\"), (\"Tulua\", \"Valle del Cauca\"),\n",
    "    (\"Independencia\", \"Valle del Cauca\"), (\"Laureano Gómez\", \"Valle del Cauca\"), (\"Quibdo\", \"Chocó\"),\n",
    "    (\"Puerto Gaitan\", \"Meta\"), (\"Pasto Las Lunas\", \"Nariño\"), (\"Ipiales\", \"Nariño\"),\n",
    "    (\"La Unión\", \"Valle del Cauca\"), (\"Pasto\", \"Nariño\"), (\"Cartagena del Chaira\", \"Caquetá\"),\n",
    "    (\"Sincelejo\", \"Sucre\"), (\"Facatativa\", \"Cundinamarca\"), (\"Engativa\", \"Cundinamarca\"),\n",
    "    (\"Tulua Calle 25\", \"Valle del Cauca\"), (\"Santa Rosa\", \"Risaralda\"), (\"Pereira Lagos\", \"Risaralda\"),\n",
    "    (\"Santa Marta\", \"Magdalena\"), (\"Duitama\", \"Boyacá\"), (\"Corozal\", \"Sucre\"), (\"Soledad\", \"Atlántico\"),\n",
    "    (\"Santander de Quilichao\", \"Cauca\"), (\"Ibague\", \"Tolima\"), (\"Santa Librada\", \"Cundinamarca\"),\n",
    "    (\"Chaparral\", \"Tolima\"), (\"Murillo Toro\", \"Tolima\"), (\"Belen\", \"Antioquia\"),\n",
    "    (\"Alfonso López\", \"Valle del Cauca\"), (\"Zipaquira\", \"Cundinamarca\"), (\"Riohacha\", \"Guajira\"),\n",
    "    (\"La Dorada\", \"Caldas\"), (\"Armenia Sur\", \"Quindío\"), (\"Calima\", \"Valle del Cauca\"),\n",
    "    (\"Fonsecca\", \"Guajira\"), (\"Sabanalarga\", \"Atlántico\"), (\"Cereté\", \"Córdoba\"),\n",
    "    (\"Rionegro\", \"Antioquia\"), (\"San Juan del Cesar\", \"Guajira\"), (\"Quebradaseca\", \"Santander\"),\n",
    "    (\"Giron\", \"Santander\"), (\"Itagui\", \"Antioquia\"), (\"Soacha\", \"Cundinamarca\"),\n",
    "    (\"Villeta\", \"Cundinamarca\"), (\"Barranquilla\", \"Atlántico\"), (\"Pradera\", \"Valle del Cauca\"),\n",
    "    (\"Patio Bonito\", \"Cundinamarca\"), (\"Chinchiná\", \"Caldas\"), (\"La Plata\", \"Huila\"),\n",
    "    (\"Autopista Norte\", \"Cundinamarca\"), (\"Jamundí\", \"Valle del Cauca\"), (\"Kennedy\", \"Cundinamarca\"),\n",
    "    (\"Sur\", \"Valle del Cauca\"), (\"Palmira\", \"Valle del Cauca\"), (\"Maicao\", \"Guajira\"),\n",
    "    (\"El Espinal\", \"Tolima\"), (\"Magangue\", \"Bolívar\"), (\"Apartado\", \"Antioquia\"),\n",
    "    (\"Calle 25\", \"Valle del Cauca\"), (\"Fundación\", \"Magdalena\"), (\"Buenaventura\", \"Valle del Cauca\")\n",
    "]]\n",
    "\n",
    "df_oficinas = pd.DataFrame(datos_oficinas, columns=[\"oficina\", \"Departamento\"])\n",
    "\n",
    "# new columns per office\n",
    "df_oficinas[\"distance_to_dto_capital\"] = df_oficinas[\"oficina\"].map(distancias_aprox)\n",
    "df_oficinas[\"is_rural\"] = df_oficinas[\"oficina\"].apply(lambda x: 1 if x in ciudades_rurales else 0)\n",
    "df_oficinas.sample(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the information about the offices to the dataframe\n",
    "\n",
    "df = df.merge(df_oficinas[[\"oficina\",\"distance_to_dto_capital\",\"is_rural\"]], on=\"oficina\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify loan term column\n",
    "\n",
    "We decided to group the loan term variable as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of different loan term values: 50\n"
     ]
    }
   ],
   "source": [
    "print(f'Current number of different loan term values: {df.plazo.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New distribution of loan term variable:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "plazo\n",
       "(12, 24]    1229\n",
       "(1, 12]      715\n",
       "(24, 36]     643\n",
       "(36, 72]     323\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.plazo = pd.cut(df.plazo,[1,12,24,36,df.plazo.max()])\n",
    "print(\"New distribution of loan term variable:\\n\")\n",
    "df.plazo.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify credit status column\n",
    "\n",
    "We decided to code the credit status variable as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current values: \n",
      "\n",
      "estado\n",
      "normal             1059\n",
      "cancelada           943\n",
      "no_especificado     605\n",
      "castigo             194\n",
      "modificado           91\n",
      "cobro judicial       13\n",
      "reestructurado        5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Current values: \\n\\n{df.estado.value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New distribution of loan term variable:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "estado\n",
       "normal             2002\n",
       "no_especificado     605\n",
       "anormal             303\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.estado = np.where(\n",
    "    df.estado.isin([\"normal\", \"cancelada\"]),\n",
    "    \"normal\",\n",
    "    np.where(df.estado.isin([\"no_especificado\"]), \"no_especificado\", \"anormal\"),\n",
    ")\n",
    "\n",
    "\n",
    "print(\"New distribution of loan term variable:\\n\")\n",
    "df.estado.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes/functions - new analyst and office columns\n",
    "\n",
    "--> In this section, we are going to define the classes and functions to create variables in the test data based on the training data, in order to avoid problems such as data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for office - percentage of risky microcredits out of total assigned by office\n",
    "\n",
    "class OfficeRiskRateTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Scikit-learn compatible transformer to calculate office risk rates.\n",
    "\n",
    "    This transformer calculates:\n",
    "    - 'office_risk_rate': Percentage of credits classified as risk levels 1, 2, or 3.\n",
    "    - 'office_high_risk_rate': Percentage of credits classified as high risk (temp_riesgo_int = 1).\n",
    "\n",
    "    The rates are computed on the training data (`fit`) and applied to new data (`transform`),\n",
    "    preventing data leakage.\n",
    "\n",
    "    Methods:\n",
    "    - fit(X, y=None): Computes risk rates per office based on the training data.\n",
    "    - transform(X): Applies the precomputed risk rates to new data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.risk_rates_ = None  # Dictionary to store precomputed risk rates\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Compute the risk rates based on the training data.\"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "\n",
    "        required_columns = {'oficina', 'temp_riesgo_int'}\n",
    "        if not required_columns.issubset(X.columns):\n",
    "            raise ValueError(f\"Input DataFrame must contain columns: {required_columns}\")\n",
    "\n",
    "        # Group by 'oficina' and 'temp_riesgo_int', then count occurrences\n",
    "        credit_count = X.groupby(['oficina', 'temp_riesgo_int']).size().unstack(fill_value=0)\n",
    "\n",
    "        # Total number of approved credits per office\n",
    "        total_credits = credit_count.sum(axis=1)\n",
    "\n",
    "        # Count credits classified as risk levels 1, 2, or 3\n",
    "        risky_credits = credit_count[[1, 2, 3]].sum(axis=1)\n",
    "\n",
    "        # Count credits classified as high risk (temp_riesgo_int = 1)\n",
    "        high_risk_credits = credit_count[1]\n",
    "\n",
    "        # Compute risk rates\n",
    "        risk_rate = (risky_credits / total_credits * 100).round(1)\n",
    "        high_risk_rate = (high_risk_credits / total_credits * 100).round(1)\n",
    "\n",
    "        # Store results in a dictionary\n",
    "        self.risk_rates_ = pd.DataFrame({\n",
    "            \"oficina\": credit_count.index,\n",
    "            \"office_risk_rate\": risk_rate,\n",
    "            \"office_high_risk_rate\": high_risk_rate\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, drop_risk=True):\n",
    "        \"\"\"Apply the precomputed risk rates to new data.\"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "\n",
    "        required_columns = {'oficina', 'temp_riesgo_int'}\n",
    "        if not required_columns.issubset(X.columns):\n",
    "            raise ValueError(f\"Input DataFrame must contain columns: {required_columns}\")\n",
    "\n",
    "        if self.risk_rates_ is None:\n",
    "            raise RuntimeError(\"The transformer has not been fitted. Call 'fit' first.\")\n",
    "\n",
    "        # Merge with the precomputed risk rates\n",
    "        X = X.merge(self.risk_rates_, left_on=\"oficina\", right_on=\"oficina\", how=\"left\")\n",
    "\n",
    "        # X = X.drop([\"oficina\"],axis=1)\n",
    "\n",
    "        if drop_risk:\n",
    "            X = X.drop([\"temp_riesgo_int\"],axis=1)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for analyst - percentage of risky microcredits out of total assigned by analyst\n",
    "\n",
    "class AnalystRiskRateTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Scikit-learn compatible transformer to calculate analyst risk rates.\n",
    "\n",
    "    This transformer calculates:\n",
    "    - 'analyst_risk_rate': Percentage of credits classified as risk levels 1, 2, or 3.\n",
    "    - 'analyst_high_risk_rate': Percentage of credits classified as high risk (temp_riesgo_int = 1).\n",
    "\n",
    "    The rates are computed on the training data (`fit`) and applied to new data (`transform`),\n",
    "    preventing data leakage.\n",
    "\n",
    "    Methods:\n",
    "    - fit(X, y=None): Computes risk rates per analyst based on the training data.\n",
    "    - transform(X): Applies the precomputed risk rates to new data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.risk_rates_ = None  # Dictionary to store precomputed risk rates\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Compute the risk rates based on the training data.\"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "\n",
    "        required_columns = {'analista', 'temp_riesgo_int'}\n",
    "        if not required_columns.issubset(X.columns):\n",
    "            raise ValueError(f\"Input DataFrame must contain columns: {required_columns}\")\n",
    "\n",
    "        # Group by 'analista' and 'temp_riesgo_int', then count occurrences\n",
    "        credit_count = X.groupby(['analista', 'temp_riesgo_int']).size().unstack(fill_value=0)\n",
    "\n",
    "        # Total number of approved credits per analyst\n",
    "        total_credits = credit_count.sum(axis=1)\n",
    "\n",
    "        # Count credits classified as risk levels 1, 2, or 3\n",
    "        risky_credits = credit_count[[1, 2, 3]].sum(axis=1)\n",
    "\n",
    "        # Count credits classified as high risk (temp_riesgo_int = 1)\n",
    "        high_risk_credits = credit_count[1]\n",
    "\n",
    "        # Compute risk rates\n",
    "        risk_rate = (risky_credits / total_credits * 100).round(1)\n",
    "        high_risk_rate = (high_risk_credits / total_credits * 100).round(1)\n",
    "\n",
    "        # Store results in a dictionary\n",
    "        self.risk_rates_ = pd.DataFrame({\n",
    "            \"analista\": credit_count.index,\n",
    "            \"analyst_risk_rate\": risk_rate,\n",
    "            \"analyst_high_risk_rate\": high_risk_rate\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, drop_risk=True):\n",
    "        \"\"\"Apply the precomputed risk rates to new data.\"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "\n",
    "        required_columns = {'analista', 'temp_riesgo_int'}\n",
    "        if not required_columns.issubset(X.columns):\n",
    "            raise ValueError(f\"Input DataFrame must contain columns: {required_columns}\")\n",
    "\n",
    "        if self.risk_rates_ is None:\n",
    "            raise RuntimeError(\"The transformer has not been fitted. Call 'fit' first.\")\n",
    "\n",
    "        # Merge with the precomputed risk rates\n",
    "        X = X.merge(self.risk_rates_, left_on=\"analista\", right_on=\"analista\", how=\"left\")\n",
    "\n",
    "        X = X.drop([\"analista\"],axis=1)\n",
    "\n",
    "        if drop_risk:\n",
    "            X = X.drop([\"temp_riesgo_int\"],axis=1)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2910 entries, 0 to 2909\n",
      "Data columns (total 20 columns):\n",
      " #   Column                          Non-Null Count  Dtype   \n",
      "---  ------                          --------------  -----   \n",
      " 0   analista                        2910 non-null   object  \n",
      " 1   tipo_credito                    2910 non-null   object  \n",
      " 2   actividad_v2                    2910 non-null   object  \n",
      " 3   monto                           2910 non-null   int64   \n",
      " 4   cuota                           2910 non-null   int64   \n",
      " 5   plazo                           2910 non-null   category\n",
      " 6   oficina                         2910 non-null   object  \n",
      " 7   zona                            2910 non-null   object  \n",
      " 8   regional                        2910 non-null   object  \n",
      " 9   estado                          2910 non-null   object  \n",
      " 10  riesgo_int                      2910 non-null   int64   \n",
      " 11  riesgo_bin                      2910 non-null   int64   \n",
      " 12  dia_semana_desembolso           2910 non-null   int32   \n",
      " 13  dia_mes_desembolso              2910 non-null   int32   \n",
      " 14  mes_desembolso                  2910 non-null   int32   \n",
      " 15  dias_desde_visita_a_desembolso  2032 non-null   float64 \n",
      " 16  desembolso_fin_de_semana        2910 non-null   int64   \n",
      " 17  desembolso_fin_de_mes           2910 non-null   int64   \n",
      " 18  distance_to_dto_capital         1981 non-null   float64 \n",
      " 19  is_rural                        1981 non-null   float64 \n",
      "dtypes: category(1), float64(3), int32(3), int64(6), object(7)\n",
      "memory usage: 401.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset split\n",
    "\n",
    "--> Taking into account the findings of the data analysis, **we proceed to make a separation of the data set between high and low amount data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers upper limit: $16,245,709.5\n",
      "Outliers lower limit: $0\n",
      "Number of microcredits consider as outliers: 355\n",
      "% of outliers of the total observations: 12.2%\n"
     ]
    }
   ],
   "source": [
    "# Q1, Q3 & IQR\n",
    "Q1 = np.percentile(df.monto, 25)  # Q1 (25%)\n",
    "Q3 = np.percentile(df.monto, 75)  # Q3 (75%)\n",
    "IQR = Q3 - Q1  # IQR\n",
    "\n",
    "# Limits for outliers\n",
    "lower_limit = Q1 - 1.5 * IQR \n",
    "lower_limit = 0 if lower_limit<0 else lower_limit\n",
    "upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outlier\n",
    "outliers = df.monto[(df.monto < lower_limit) | (df.monto > upper_limit)]\n",
    "upper_outliers = df[(df.monto > upper_limit)]\n",
    "\n",
    "print(f'Outliers upper limit: ${upper_limit:,}')\n",
    "print(f'Outliers lower limit: ${lower_limit:,}')\n",
    "\n",
    "print(f'Number of microcredits consider as outliers: {upper_outliers.shape[0]}')\n",
    "print(f'% of outliers of the total observations: {round(upper_outliers.shape[0]/df.shape[0]*100,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, if we split the data by this outlier criterion, **we would be left with a dataset of less than 400 observations**, which may affect the performance of the model of high loan amounts. \n",
    "\n",
    "Because of this, **we propose to split the datasets based on the value of a percentile amount chosen by the team**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the amount percentile of P75, the amount limit should be: $7,605,795\n",
      "\n",
      "The total number of observations for the dataset with lower loan amounts are: 2182\n",
      "The total number of observations for the dataset with higher loan amounts are: 728\n"
     ]
    }
   ],
   "source": [
    "amount_percentile = 75\n",
    "\n",
    "p = np.percentile(df.monto, amount_percentile)\n",
    "print(\n",
    "    f\"Using the amount percentile of P{amount_percentile}, the amount limit should be: ${round(p):,}\"\n",
    ")\n",
    "\n",
    "df_total = df.copy()\n",
    "df_low_amount = df[df[\"monto\"] < p]\n",
    "df_high_amount = df[df[\"monto\"] >= p]\n",
    "\n",
    "print()\n",
    "\n",
    "print(\n",
    "    f\"The total number of observations for the dataset with lower loan amounts are: {df_low_amount.shape[0]}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The total number of observations for the dataset with higher loan amounts are: {df_high_amount.shape[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets for modelling\n",
    "\n",
    "--> We decide to develop models testing a binary objective variable and a multiclass objective variable.  \n",
    "--> In this section we finish **applying the separation into training and testing, creation of last variables, selection of variables and coding or scaling of variables.**  \n",
    "\n",
    "--> We decided to apply the following train test split criteria, to avoiding the ***curse of dimensionality***:\n",
    "- Low amount data: 75-25\n",
    "- High amount data: 70-30  --- Also, in this dataset, we avoid the one hot encoding of the \"oficina\" variable\n",
    "- All the data: 80-20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary target variable\n",
    "\n",
    "Droping the multiclass objective variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_low = df_low_amount.copy()\n",
    "temp_high = df_high_amount.copy()\n",
    "temp_total = df_total.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### low amount data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop multiclass objective variable\n",
    "try:\n",
    "    temp_low[\"temp_riesgo_int\"] = temp_low[\"riesgo_int\"] # this variable is important in next steps\n",
    "    temp_low = temp_low.drop(\"riesgo_int\", axis=1)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 2. X & y variables\n",
    "X = temp_low.drop(\"riesgo_bin\", axis=1)\n",
    "y = temp_low[\"riesgo_bin\"]\n",
    "\n",
    "# 3. Log transformation for the loan amount and loan payments variables - due to their skewness distributions\n",
    "X[\"monto\"] = np.log(X[\"monto\"])\n",
    "X[\"cuota\"] = np.log(X[\"cuota\"])\n",
    "\n",
    "# 4. train & test split\n",
    "X_train_low_bin, X_test_low_bin, y_train_low_bin, y_test_low_bin = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets add some new features related to the analyst and office proportons of risk credits in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. creation or transformation of variables, avoiding data leakage\n",
    "\n",
    "# initialize transformers\n",
    "analyst_transformer = AnalystRiskRateTransformer()\n",
    "office_transformer = OfficeRiskRateTransformer()\n",
    "\n",
    "# train and transform training set\n",
    "\n",
    "# new analyst tags\n",
    "X_train_low_bin = analyst_transformer.fit(X_train_low_bin).transform(X_train_low_bin, drop_risk=False)\n",
    "X_test_low_bin = analyst_transformer.transform(X_test_low_bin, drop_risk=False)\n",
    "\n",
    "#new office tags\n",
    "X_train_low_bin = office_transformer.fit(X_train_low_bin).transform(X_train_low_bin, drop_risk=True)\n",
    "X_test_low_bin = office_transformer.transform(X_test_low_bin, drop_risk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. One hot encoding and scaling of variables\n",
    "\n",
    "# One hot encoding of categorical and object variables -- Min max scaler of numerical variables\n",
    "categorical_features = X_train_low_bin.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "numerical_features = X_train_low_bin.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\"]).columns.tolist()\n",
    "\n",
    "# The one hot encoding drop the first level to avoid multicolineality\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", MinMaxScaler(), numerical_features),  # MinMax\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False), categorical_features),  # One-Hot Encoding\n",
    "    ],\n",
    "    remainder=\"passthrough\" \n",
    ")\n",
    "\n",
    "# Fit the preprocessor only with the training set to avoid data leakage.\n",
    "X_train_tranformed = preprocessor.fit_transform(X_train_low_bin)\n",
    "X_test_transformed = preprocessor.transform(X_test_low_bin)\n",
    "\n",
    "# Obtain feature names after transformation\n",
    "encoded_features = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_features)\n",
    "column_names = numerical_features + encoded_features.tolist()\n",
    "\n",
    "# Convert the results back to DataFrame\n",
    "X_train_low_bin = pd.DataFrame(X_train_tranformed, columns=column_names, index=X_train_low_bin.index)\n",
    "X_test_low_bin = pd.DataFrame(X_test_transformed, columns=column_names, index=X_test_low_bin.index)\n",
    "\n",
    "del X_train_tranformed, X_test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1636 entries, 0 to 1635\n",
      "Columns: 119 entries, monto to estado_normal\n",
      "dtypes: float64(119)\n",
      "memory usage: 1.5 MB\n"
     ]
    }
   ],
   "source": [
    "X_train_low_bin.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 546 entries, 0 to 545\n",
      "Columns: 119 entries, monto to estado_normal\n",
      "dtypes: float64(119)\n",
      "memory usage: 507.7 KB\n"
     ]
    }
   ],
   "source": [
    "X_test_low_bin.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_low_bin.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High amount data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop multiclass objective variable\n",
    "try:\n",
    "    temp_high[\"temp_riesgo_int\"] = temp_high[\"riesgo_int\"] # this variable is important in next steps\n",
    "    temp_high = temp_high.drop(\"riesgo_int\", axis=1)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 2. X & y variables\n",
    "X = temp_high.drop(\"riesgo_bin\", axis=1)\n",
    "y = temp_high[\"riesgo_bin\"]\n",
    "\n",
    "# 3. Log transformation for the loan amount and loan payments variables - due to their skewness distributions\n",
    "X[\"monto\"] = np.log(X[\"monto\"])\n",
    "X[\"cuota\"] = np.log(X[\"cuota\"])\n",
    "\n",
    "# 4. train & test split\n",
    "X_train_high_bin, X_test_high_bin, y_train_high_bin, y_test_high_bin = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets add some new features related to the analyst and office proportons of risk credits in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. creation or transformation of variables, avoiding data leakage\n",
    "\n",
    "# initialize transformers\n",
    "analyst_transformer = AnalystRiskRateTransformer()\n",
    "office_transformer = OfficeRiskRateTransformer()\n",
    "\n",
    "# train and transform training set\n",
    "\n",
    "# new analyst tags\n",
    "X_train_high_bin = analyst_transformer.fit(X_train_high_bin).transform(X_train_high_bin, drop_risk=False)\n",
    "X_test_high_bin = analyst_transformer.transform(X_test_high_bin, drop_risk=False)\n",
    "\n",
    "#new office tags -- AND DROP THE ORIGINAL OFFICE COLUMN\n",
    "X_train_high_bin = office_transformer.fit(X_train_high_bin).transform(X_train_high_bin, drop_risk=True).drop(\"oficina\",axis=1)\n",
    "X_test_high_bin = office_transformer.transform(X_test_high_bin, drop_risk=True).drop(\"oficina\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. One hot encoding and scaling of variables\n",
    "\n",
    "# One hot encoding of categorical and object variables -- Min max scaler of numerical variables\n",
    "categorical_features = X_train_high_bin.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "numerical_features = X_train_high_bin.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\"]).columns.tolist()\n",
    "\n",
    "# The one hot encoding drop the first level to avoid multicolineality\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", MinMaxScaler(), numerical_features),  # MinMax\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False), categorical_features),  # One-Hot Encoding\n",
    "    ],\n",
    "    remainder=\"passthrough\" \n",
    ")\n",
    "\n",
    "# Fit the preprocessor only with the training set to avoid data leakage.\n",
    "X_train_tranformed = preprocessor.fit_transform(X_train_high_bin)\n",
    "X_test_transformed = preprocessor.transform(X_test_high_bin)\n",
    "\n",
    "# Obtain feature names after transformation\n",
    "encoded_features = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_features)\n",
    "column_names = numerical_features + encoded_features.tolist()\n",
    "\n",
    "# Convert the results back to DataFrame\n",
    "X_train_high_bin = pd.DataFrame(X_train_tranformed, columns=column_names, index=X_train_high_bin.index)\n",
    "X_test_high_bin = pd.DataFrame(X_test_transformed, columns=column_names, index=X_test_high_bin.index)\n",
    "\n",
    "del X_train_tranformed, X_test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 509 entries, 0 to 508\n",
      "Data columns (total 59 columns):\n",
      " #   Column                                           Non-Null Count  Dtype  \n",
      "---  ------                                           --------------  -----  \n",
      " 0   monto                                            509 non-null    float64\n",
      " 1   cuota                                            509 non-null    float64\n",
      " 2   dia_semana_desembolso                            509 non-null    float64\n",
      " 3   dia_mes_desembolso                               509 non-null    float64\n",
      " 4   mes_desembolso                                   509 non-null    float64\n",
      " 5   dias_desde_visita_a_desembolso                   348 non-null    float64\n",
      " 6   desembolso_fin_de_semana                         509 non-null    float64\n",
      " 7   desembolso_fin_de_mes                            509 non-null    float64\n",
      " 8   distance_to_dto_capital                          350 non-null    float64\n",
      " 9   is_rural                                         350 non-null    float64\n",
      " 10  analyst_risk_rate                                509 non-null    float64\n",
      " 11  analyst_high_risk_rate                           509 non-null    float64\n",
      " 12  office_risk_rate                                 509 non-null    float64\n",
      " 13  office_high_risk_rate                            509 non-null    float64\n",
      " 14  tipo_credito_Nuevo                               509 non-null    float64\n",
      " 15  tipo_credito_Preferencial                        509 non-null    float64\n",
      " 16  tipo_credito_Renovacion                          509 non-null    float64\n",
      " 17  actividad_v2_arriendos_alquiler_e_inmobiliarios  509 non-null    float64\n",
      " 18  actividad_v2_belleza_y_estetica                  509 non-null    float64\n",
      " 19  actividad_v2_comercio_ambulante                  509 non-null    float64\n",
      " 20  actividad_v2_comercio_y_ventas_general           509 non-null    float64\n",
      " 21  actividad_v2_comercios_varios_y_detallistas      509 non-null    float64\n",
      " 22  actividad_v2_confeccion_y_afines                 509 non-null    float64\n",
      " 23  actividad_v2_construccion_obras_y_afines         509 non-null    float64\n",
      " 24  actividad_v2_lavaderos_parqueaderos_y_afines     509 non-null    float64\n",
      " 25  actividad_v2_no_especificado                     509 non-null    float64\n",
      " 26  actividad_v2_oficios_tecnicos_y_manuales         509 non-null    float64\n",
      " 27  actividad_v2_otros_servicios_y_negocios          509 non-null    float64\n",
      " 28  actividad_v2_salud_y_afines                      509 non-null    float64\n",
      " 29  actividad_v2_sector_alimenticio                  509 non-null    float64\n",
      " 30  actividad_v2_servicios_de_limpieza               509 non-null    float64\n",
      " 31  actividad_v2_servicios_educativos                509 non-null    float64\n",
      " 32  actividad_v2_tiendas_almacenes_y_ferreterias     509 non-null    float64\n",
      " 33  actividad_v2_transporte_vehiculos_y_afines       509 non-null    float64\n",
      " 34  plazo_(12, 24]                                   509 non-null    float64\n",
      " 35  plazo_(24, 36]                                   509 non-null    float64\n",
      " 36  plazo_(36, 72]                                   509 non-null    float64\n",
      " 37  zona_2                                           509 non-null    float64\n",
      " 38  zona_3                                           509 non-null    float64\n",
      " 39  zona_4                                           509 non-null    float64\n",
      " 40  zona_5                                           509 non-null    float64\n",
      " 41  zona_6                                           509 non-null    float64\n",
      " 42  zona_7                                           509 non-null    float64\n",
      " 43  zona_8                                           509 non-null    float64\n",
      " 44  zona_9                                           509 non-null    float64\n",
      " 45  zona_10                                          509 non-null    float64\n",
      " 46  zona_11                                          509 non-null    float64\n",
      " 47  zona_12                                          509 non-null    float64\n",
      " 48  zona_13                                          509 non-null    float64\n",
      " 49  zona_14                                          509 non-null    float64\n",
      " 50  zona_15                                          509 non-null    float64\n",
      " 51  zona_16                                          509 non-null    float64\n",
      " 52  zona_17                                          509 non-null    float64\n",
      " 53  regional_2                                       509 non-null    float64\n",
      " 54  regional_3                                       509 non-null    float64\n",
      " 55  regional_4                                       509 non-null    float64\n",
      " 56  regional_5                                       509 non-null    float64\n",
      " 57  estado_no_especificado                           509 non-null    float64\n",
      " 58  estado_normal                                    509 non-null    float64\n",
      "dtypes: float64(59)\n",
      "memory usage: 234.7 KB\n"
     ]
    }
   ],
   "source": [
    "X_train_high_bin.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 219 entries, 0 to 218\n",
      "Data columns (total 59 columns):\n",
      " #   Column                                           Non-Null Count  Dtype  \n",
      "---  ------                                           --------------  -----  \n",
      " 0   monto                                            219 non-null    float64\n",
      " 1   cuota                                            219 non-null    float64\n",
      " 2   dia_semana_desembolso                            219 non-null    float64\n",
      " 3   dia_mes_desembolso                               219 non-null    float64\n",
      " 4   mes_desembolso                                   219 non-null    float64\n",
      " 5   dias_desde_visita_a_desembolso                   144 non-null    float64\n",
      " 6   desembolso_fin_de_semana                         219 non-null    float64\n",
      " 7   desembolso_fin_de_mes                            219 non-null    float64\n",
      " 8   distance_to_dto_capital                          155 non-null    float64\n",
      " 9   is_rural                                         155 non-null    float64\n",
      " 10  analyst_risk_rate                                170 non-null    float64\n",
      " 11  analyst_high_risk_rate                           170 non-null    float64\n",
      " 12  office_risk_rate                                 219 non-null    float64\n",
      " 13  office_high_risk_rate                            219 non-null    float64\n",
      " 14  tipo_credito_Nuevo                               219 non-null    float64\n",
      " 15  tipo_credito_Preferencial                        219 non-null    float64\n",
      " 16  tipo_credito_Renovacion                          219 non-null    float64\n",
      " 17  actividad_v2_arriendos_alquiler_e_inmobiliarios  219 non-null    float64\n",
      " 18  actividad_v2_belleza_y_estetica                  219 non-null    float64\n",
      " 19  actividad_v2_comercio_ambulante                  219 non-null    float64\n",
      " 20  actividad_v2_comercio_y_ventas_general           219 non-null    float64\n",
      " 21  actividad_v2_comercios_varios_y_detallistas      219 non-null    float64\n",
      " 22  actividad_v2_confeccion_y_afines                 219 non-null    float64\n",
      " 23  actividad_v2_construccion_obras_y_afines         219 non-null    float64\n",
      " 24  actividad_v2_lavaderos_parqueaderos_y_afines     219 non-null    float64\n",
      " 25  actividad_v2_no_especificado                     219 non-null    float64\n",
      " 26  actividad_v2_oficios_tecnicos_y_manuales         219 non-null    float64\n",
      " 27  actividad_v2_otros_servicios_y_negocios          219 non-null    float64\n",
      " 28  actividad_v2_salud_y_afines                      219 non-null    float64\n",
      " 29  actividad_v2_sector_alimenticio                  219 non-null    float64\n",
      " 30  actividad_v2_servicios_de_limpieza               219 non-null    float64\n",
      " 31  actividad_v2_servicios_educativos                219 non-null    float64\n",
      " 32  actividad_v2_tiendas_almacenes_y_ferreterias     219 non-null    float64\n",
      " 33  actividad_v2_transporte_vehiculos_y_afines       219 non-null    float64\n",
      " 34  plazo_(12, 24]                                   219 non-null    float64\n",
      " 35  plazo_(24, 36]                                   219 non-null    float64\n",
      " 36  plazo_(36, 72]                                   219 non-null    float64\n",
      " 37  zona_2                                           219 non-null    float64\n",
      " 38  zona_3                                           219 non-null    float64\n",
      " 39  zona_4                                           219 non-null    float64\n",
      " 40  zona_5                                           219 non-null    float64\n",
      " 41  zona_6                                           219 non-null    float64\n",
      " 42  zona_7                                           219 non-null    float64\n",
      " 43  zona_8                                           219 non-null    float64\n",
      " 44  zona_9                                           219 non-null    float64\n",
      " 45  zona_10                                          219 non-null    float64\n",
      " 46  zona_11                                          219 non-null    float64\n",
      " 47  zona_12                                          219 non-null    float64\n",
      " 48  zona_13                                          219 non-null    float64\n",
      " 49  zona_14                                          219 non-null    float64\n",
      " 50  zona_15                                          219 non-null    float64\n",
      " 51  zona_16                                          219 non-null    float64\n",
      " 52  zona_17                                          219 non-null    float64\n",
      " 53  regional_2                                       219 non-null    float64\n",
      " 54  regional_3                                       219 non-null    float64\n",
      " 55  regional_4                                       219 non-null    float64\n",
      " 56  regional_5                                       219 non-null    float64\n",
      " 57  estado_no_especificado                           219 non-null    float64\n",
      " 58  estado_normal                                    219 non-null    float64\n",
      "dtypes: float64(59)\n",
      "memory usage: 101.1 KB\n"
     ]
    }
   ],
   "source": [
    "X_test_high_bin.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop multiclass objective variable\n",
    "try:\n",
    "    temp_total[\"temp_riesgo_int\"] = temp_total[\"riesgo_int\"] # this variable is important in next steps\n",
    "    temp_total = temp_total.drop(\"riesgo_int\", axis=1)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 2. X & y variables\n",
    "X = temp_total.drop(\"riesgo_bin\", axis=1)\n",
    "y = temp_total[\"riesgo_bin\"]\n",
    "\n",
    "# 3. Log transformation for the loan amount and loan payments variables - due to their skewness distributions\n",
    "X[\"monto\"] = np.log(X[\"monto\"])\n",
    "X[\"cuota\"] = np.log(X[\"cuota\"])\n",
    "\n",
    "# 4. train & test split\n",
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets add some new features related to the analyst and office proportons of risk credits in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. creation or transformation of variables, avoiding data leakage\n",
    "\n",
    "# initialize transformers\n",
    "analyst_transformer = AnalystRiskRateTransformer()\n",
    "office_transformer = OfficeRiskRateTransformer()\n",
    "\n",
    "# train and transform training set\n",
    "\n",
    "# new analyst tags\n",
    "X_train_bin = analyst_transformer.fit(X_train_bin).transform(X_train_bin, drop_risk=False)\n",
    "X_test_bin = analyst_transformer.transform(X_test_bin, drop_risk=False)\n",
    "\n",
    "#new office tags\n",
    "X_train_bin = office_transformer.fit(X_train_bin).transform(X_train_bin, drop_risk=True)\n",
    "X_test_bin = office_transformer.transform(X_test_bin, drop_risk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. One hot encoding and scaling of variables\n",
    "\n",
    "# One hot encoding of categorical and object variables -- Min max scaler of numerical variables\n",
    "categorical_features = X_train_bin.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "numerical_features = X_train_bin.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\"]).columns.tolist()\n",
    "\n",
    "# The one hot encoding drop the first level to avoid multicolineality\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", MinMaxScaler(), numerical_features),  # MinMax\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False), categorical_features),  # One-Hot Encoding\n",
    "    ],\n",
    "    remainder=\"passthrough\" \n",
    ")\n",
    "\n",
    "# Fit the preprocessor only with the training set to avoid data leakage.\n",
    "X_train_tranformed = preprocessor.fit_transform(X_train_bin)\n",
    "X_test_transformed = preprocessor.transform(X_test_bin)\n",
    "\n",
    "# Obtain feature names after transformation\n",
    "encoded_features = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_features)\n",
    "column_names = numerical_features + encoded_features.tolist()\n",
    "\n",
    "# Convert the results back to DataFrame\n",
    "X_train_bin = pd.DataFrame(X_train_tranformed, columns=column_names, index=X_train_bin.index)\n",
    "X_test_bin = pd.DataFrame(X_test_transformed, columns=column_names, index=X_test_bin.index)\n",
    "\n",
    "del X_train_tranformed, X_test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2328 entries, 0 to 2327\n",
      "Columns: 119 entries, monto to estado_normal\n",
      "dtypes: float64(119)\n",
      "memory usage: 2.1 MB\n"
     ]
    }
   ],
   "source": [
    "X_train_bin.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 582 entries, 0 to 581\n",
      "Columns: 119 entries, monto to estado_normal\n",
      "dtypes: float64(119)\n",
      "memory usage: 541.2 KB\n"
     ]
    }
   ],
   "source": [
    "X_test_bin.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass target variable\n",
    "\n",
    "Droping the binary objective variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_low = df_low_amount.copy()\n",
    "temp_high = df_high_amount.copy()\n",
    "temp_total = df_total.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### low amount data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop binary objective variable\n",
    "try:\n",
    "    temp_low[\"temp_riesgo_int\"] = temp_low[\"riesgo_int\"] # this variable is important in next steps\n",
    "    temp_low = temp_low.drop(\"riesgo_bin\", axis=1)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 2. X & y variables\n",
    "X = temp_low.drop(\"riesgo_int\", axis=1)\n",
    "y = temp_low[\"riesgo_int\"]\n",
    "\n",
    "# 3. Log transformation for the loan amount and loan payments variables - due to their skewness distributions\n",
    "X[\"monto\"] = np.log(X[\"monto\"])\n",
    "X[\"cuota\"] = np.log(X[\"cuota\"])\n",
    "\n",
    "# 4. train & test split\n",
    "X_train_low_multi, X_test_low_multi, y_train_low_multi, y_test_low_multi = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets add some new features related to the analyst and office proportons of risk credits in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. creation or transformation of variables, avoiding data leakage\n",
    "\n",
    "# initialize transformers\n",
    "analyst_transformer = AnalystRiskRateTransformer()\n",
    "office_transformer = OfficeRiskRateTransformer()\n",
    "\n",
    "# train and transform training set\n",
    "\n",
    "# new analyst tags\n",
    "X_train_low_multi = analyst_transformer.fit(X_train_low_multi).transform(X_train_low_multi, drop_risk=False)\n",
    "X_test_low_multi = analyst_transformer.transform(X_test_low_multi, drop_risk=False)\n",
    "\n",
    "#new office tags\n",
    "X_train_low_multi = office_transformer.fit(X_train_low_multi).transform(X_train_low_multi, drop_risk=True)\n",
    "X_test_low_multi = office_transformer.transform(X_test_low_multi, drop_risk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. One hot encoding and scaling of variables\n",
    "\n",
    "# One hot encoding of categorical and object variables -- Min max scaler of numerical variables\n",
    "categorical_features = X_train_low_multi.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "numerical_features = X_train_low_multi.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\"]).columns.tolist()\n",
    "\n",
    "# The one hot encoding drop the first level to avoid multicolineality\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", MinMaxScaler(), numerical_features),  # MinMax\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False), categorical_features),  # One-Hot Encoding\n",
    "    ],\n",
    "    remainder=\"passthrough\" \n",
    ")\n",
    "\n",
    "# Fit the preprocessor only with the training set to avoid data leakage.\n",
    "X_train_tranformed = preprocessor.fit_transform(X_train_low_multi)\n",
    "X_test_transformed = preprocessor.transform(X_test_low_multi)\n",
    "\n",
    "# Obtain feature names after transformation\n",
    "encoded_features = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_features)\n",
    "column_names = numerical_features + encoded_features.tolist()\n",
    "\n",
    "# Convert the results back to DataFrame\n",
    "X_train_low_multi = pd.DataFrame(X_train_tranformed, columns=column_names, index=X_train_low_multi.index)\n",
    "X_test_low_multi = pd.DataFrame(X_test_transformed, columns=column_names, index=X_test_low_multi.index)\n",
    "\n",
    "del X_train_tranformed, X_test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1636 entries, 0 to 1635\n",
      "Columns: 119 entries, monto to estado_normal\n",
      "dtypes: float64(119)\n",
      "memory usage: 1.5 MB\n"
     ]
    }
   ],
   "source": [
    "X_train_low_multi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 546 entries, 0 to 545\n",
      "Columns: 119 entries, monto to estado_normal\n",
      "dtypes: float64(119)\n",
      "memory usage: 507.7 KB\n"
     ]
    }
   ],
   "source": [
    "X_test_low_multi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_low_bin.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High amount data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop binary objective variable\n",
    "try:\n",
    "    temp_high[\"temp_riesgo_int\"] = temp_high[\"riesgo_int\"] # this variable is important in next steps\n",
    "    temp_high = temp_high.drop(\"riesgo_bin\", axis=1)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 2. X & y variables\n",
    "X = temp_high.drop(\"riesgo_int\", axis=1)\n",
    "y = temp_high[\"riesgo_int\"]\n",
    "\n",
    "# 3. Log transformation for the loan amount and loan payments variables - due to their skewness distributions\n",
    "X[\"monto\"] = np.log(X[\"monto\"])\n",
    "X[\"cuota\"] = np.log(X[\"cuota\"])\n",
    "\n",
    "# 4. train & test split\n",
    "X_train_high_multi, X_test_high_multi, y_train_high_multi, y_test_high_multi = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets add some new features related to the analyst and office proportons of risk credits in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. creation or transformation of variables, avoiding data leakage\n",
    "\n",
    "# initialize transformers\n",
    "analyst_transformer = AnalystRiskRateTransformer()\n",
    "office_transformer = OfficeRiskRateTransformer()\n",
    "\n",
    "# train and transform training set\n",
    "\n",
    "# new analyst tags\n",
    "X_train_high_multi = analyst_transformer.fit(X_train_high_multi).transform(X_train_high_multi, drop_risk=False)\n",
    "X_test_high_multi = analyst_transformer.transform(X_test_high_multi, drop_risk=False)\n",
    "\n",
    "#new office tags -- AND DROP THE ORIGINAL OFFICE COLUMN\n",
    "X_train_high_multi = office_transformer.fit(X_train_high_multi).transform(X_train_high_multi, drop_risk=True).drop(\"oficina\",axis=1)\n",
    "X_test_high_multi = office_transformer.transform(X_test_high_multi, drop_risk=True).drop(\"oficina\",axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. One hot encoding and scaling of variables\n",
    "\n",
    "# One hot encoding of categorical and object variables -- Min max scaler of numerical variables\n",
    "categorical_features = X_train_high_multi.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "numerical_features = X_train_high_multi.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\"]).columns.tolist()\n",
    "\n",
    "# The one hot encoding drop the first level to avoid multicolineality\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", MinMaxScaler(), numerical_features),  # MinMax\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False), categorical_features),  # One-Hot Encoding\n",
    "    ],\n",
    "    remainder=\"passthrough\" \n",
    ")\n",
    "\n",
    "# Fit the preprocessor only with the training set to avoid data leakage.\n",
    "X_train_tranformed = preprocessor.fit_transform(X_train_high_multi)\n",
    "X_test_transformed = preprocessor.transform(X_test_high_multi)\n",
    "\n",
    "# Obtain feature names after transformation\n",
    "encoded_features = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_features)\n",
    "column_names = numerical_features + encoded_features.tolist()\n",
    "\n",
    "# Convert the results back to DataFrame\n",
    "X_train_high_multi = pd.DataFrame(X_train_tranformed, columns=column_names, index=X_train_high_multi.index)\n",
    "X_test_high_multi = pd.DataFrame(X_test_transformed, columns=column_names, index=X_test_high_multi.index)\n",
    "\n",
    "del X_train_tranformed, X_test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 509 entries, 0 to 508\n",
      "Data columns (total 59 columns):\n",
      " #   Column                                           Non-Null Count  Dtype  \n",
      "---  ------                                           --------------  -----  \n",
      " 0   monto                                            509 non-null    float64\n",
      " 1   cuota                                            509 non-null    float64\n",
      " 2   dia_semana_desembolso                            509 non-null    float64\n",
      " 3   dia_mes_desembolso                               509 non-null    float64\n",
      " 4   mes_desembolso                                   509 non-null    float64\n",
      " 5   dias_desde_visita_a_desembolso                   348 non-null    float64\n",
      " 6   desembolso_fin_de_semana                         509 non-null    float64\n",
      " 7   desembolso_fin_de_mes                            509 non-null    float64\n",
      " 8   distance_to_dto_capital                          350 non-null    float64\n",
      " 9   is_rural                                         350 non-null    float64\n",
      " 10  analyst_risk_rate                                509 non-null    float64\n",
      " 11  analyst_high_risk_rate                           509 non-null    float64\n",
      " 12  office_risk_rate                                 509 non-null    float64\n",
      " 13  office_high_risk_rate                            509 non-null    float64\n",
      " 14  tipo_credito_Nuevo                               509 non-null    float64\n",
      " 15  tipo_credito_Preferencial                        509 non-null    float64\n",
      " 16  tipo_credito_Renovacion                          509 non-null    float64\n",
      " 17  actividad_v2_arriendos_alquiler_e_inmobiliarios  509 non-null    float64\n",
      " 18  actividad_v2_belleza_y_estetica                  509 non-null    float64\n",
      " 19  actividad_v2_comercio_ambulante                  509 non-null    float64\n",
      " 20  actividad_v2_comercio_y_ventas_general           509 non-null    float64\n",
      " 21  actividad_v2_comercios_varios_y_detallistas      509 non-null    float64\n",
      " 22  actividad_v2_confeccion_y_afines                 509 non-null    float64\n",
      " 23  actividad_v2_construccion_obras_y_afines         509 non-null    float64\n",
      " 24  actividad_v2_lavaderos_parqueaderos_y_afines     509 non-null    float64\n",
      " 25  actividad_v2_no_especificado                     509 non-null    float64\n",
      " 26  actividad_v2_oficios_tecnicos_y_manuales         509 non-null    float64\n",
      " 27  actividad_v2_otros_servicios_y_negocios          509 non-null    float64\n",
      " 28  actividad_v2_salud_y_afines                      509 non-null    float64\n",
      " 29  actividad_v2_sector_alimenticio                  509 non-null    float64\n",
      " 30  actividad_v2_servicios_de_limpieza               509 non-null    float64\n",
      " 31  actividad_v2_servicios_educativos                509 non-null    float64\n",
      " 32  actividad_v2_tiendas_almacenes_y_ferreterias     509 non-null    float64\n",
      " 33  actividad_v2_transporte_vehiculos_y_afines       509 non-null    float64\n",
      " 34  plazo_(12, 24]                                   509 non-null    float64\n",
      " 35  plazo_(24, 36]                                   509 non-null    float64\n",
      " 36  plazo_(36, 72]                                   509 non-null    float64\n",
      " 37  zona_2                                           509 non-null    float64\n",
      " 38  zona_3                                           509 non-null    float64\n",
      " 39  zona_4                                           509 non-null    float64\n",
      " 40  zona_5                                           509 non-null    float64\n",
      " 41  zona_6                                           509 non-null    float64\n",
      " 42  zona_7                                           509 non-null    float64\n",
      " 43  zona_8                                           509 non-null    float64\n",
      " 44  zona_9                                           509 non-null    float64\n",
      " 45  zona_10                                          509 non-null    float64\n",
      " 46  zona_11                                          509 non-null    float64\n",
      " 47  zona_12                                          509 non-null    float64\n",
      " 48  zona_13                                          509 non-null    float64\n",
      " 49  zona_14                                          509 non-null    float64\n",
      " 50  zona_15                                          509 non-null    float64\n",
      " 51  zona_16                                          509 non-null    float64\n",
      " 52  zona_17                                          509 non-null    float64\n",
      " 53  regional_2                                       509 non-null    float64\n",
      " 54  regional_3                                       509 non-null    float64\n",
      " 55  regional_4                                       509 non-null    float64\n",
      " 56  regional_5                                       509 non-null    float64\n",
      " 57  estado_no_especificado                           509 non-null    float64\n",
      " 58  estado_normal                                    509 non-null    float64\n",
      "dtypes: float64(59)\n",
      "memory usage: 234.7 KB\n"
     ]
    }
   ],
   "source": [
    "X_train_high_multi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 219 entries, 0 to 218\n",
      "Data columns (total 59 columns):\n",
      " #   Column                                           Non-Null Count  Dtype  \n",
      "---  ------                                           --------------  -----  \n",
      " 0   monto                                            219 non-null    float64\n",
      " 1   cuota                                            219 non-null    float64\n",
      " 2   dia_semana_desembolso                            219 non-null    float64\n",
      " 3   dia_mes_desembolso                               219 non-null    float64\n",
      " 4   mes_desembolso                                   219 non-null    float64\n",
      " 5   dias_desde_visita_a_desembolso                   144 non-null    float64\n",
      " 6   desembolso_fin_de_semana                         219 non-null    float64\n",
      " 7   desembolso_fin_de_mes                            219 non-null    float64\n",
      " 8   distance_to_dto_capital                          155 non-null    float64\n",
      " 9   is_rural                                         155 non-null    float64\n",
      " 10  analyst_risk_rate                                170 non-null    float64\n",
      " 11  analyst_high_risk_rate                           170 non-null    float64\n",
      " 12  office_risk_rate                                 219 non-null    float64\n",
      " 13  office_high_risk_rate                            219 non-null    float64\n",
      " 14  tipo_credito_Nuevo                               219 non-null    float64\n",
      " 15  tipo_credito_Preferencial                        219 non-null    float64\n",
      " 16  tipo_credito_Renovacion                          219 non-null    float64\n",
      " 17  actividad_v2_arriendos_alquiler_e_inmobiliarios  219 non-null    float64\n",
      " 18  actividad_v2_belleza_y_estetica                  219 non-null    float64\n",
      " 19  actividad_v2_comercio_ambulante                  219 non-null    float64\n",
      " 20  actividad_v2_comercio_y_ventas_general           219 non-null    float64\n",
      " 21  actividad_v2_comercios_varios_y_detallistas      219 non-null    float64\n",
      " 22  actividad_v2_confeccion_y_afines                 219 non-null    float64\n",
      " 23  actividad_v2_construccion_obras_y_afines         219 non-null    float64\n",
      " 24  actividad_v2_lavaderos_parqueaderos_y_afines     219 non-null    float64\n",
      " 25  actividad_v2_no_especificado                     219 non-null    float64\n",
      " 26  actividad_v2_oficios_tecnicos_y_manuales         219 non-null    float64\n",
      " 27  actividad_v2_otros_servicios_y_negocios          219 non-null    float64\n",
      " 28  actividad_v2_salud_y_afines                      219 non-null    float64\n",
      " 29  actividad_v2_sector_alimenticio                  219 non-null    float64\n",
      " 30  actividad_v2_servicios_de_limpieza               219 non-null    float64\n",
      " 31  actividad_v2_servicios_educativos                219 non-null    float64\n",
      " 32  actividad_v2_tiendas_almacenes_y_ferreterias     219 non-null    float64\n",
      " 33  actividad_v2_transporte_vehiculos_y_afines       219 non-null    float64\n",
      " 34  plazo_(12, 24]                                   219 non-null    float64\n",
      " 35  plazo_(24, 36]                                   219 non-null    float64\n",
      " 36  plazo_(36, 72]                                   219 non-null    float64\n",
      " 37  zona_2                                           219 non-null    float64\n",
      " 38  zona_3                                           219 non-null    float64\n",
      " 39  zona_4                                           219 non-null    float64\n",
      " 40  zona_5                                           219 non-null    float64\n",
      " 41  zona_6                                           219 non-null    float64\n",
      " 42  zona_7                                           219 non-null    float64\n",
      " 43  zona_8                                           219 non-null    float64\n",
      " 44  zona_9                                           219 non-null    float64\n",
      " 45  zona_10                                          219 non-null    float64\n",
      " 46  zona_11                                          219 non-null    float64\n",
      " 47  zona_12                                          219 non-null    float64\n",
      " 48  zona_13                                          219 non-null    float64\n",
      " 49  zona_14                                          219 non-null    float64\n",
      " 50  zona_15                                          219 non-null    float64\n",
      " 51  zona_16                                          219 non-null    float64\n",
      " 52  zona_17                                          219 non-null    float64\n",
      " 53  regional_2                                       219 non-null    float64\n",
      " 54  regional_3                                       219 non-null    float64\n",
      " 55  regional_4                                       219 non-null    float64\n",
      " 56  regional_5                                       219 non-null    float64\n",
      " 57  estado_no_especificado                           219 non-null    float64\n",
      " 58  estado_normal                                    219 non-null    float64\n",
      "dtypes: float64(59)\n",
      "memory usage: 101.1 KB\n"
     ]
    }
   ],
   "source": [
    "X_test_high_multi.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop binary objective variable\n",
    "try:\n",
    "    temp_total[\"temp_riesgo_int\"] = temp_total[\"riesgo_int\"] # this variable is important in next steps\n",
    "    temp_total = temp_total.drop(\"riesgo_bin\", axis=1)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 2. X & y variables\n",
    "X = temp_total.drop(\"riesgo_int\", axis=1)\n",
    "y = temp_total[\"riesgo_int\"]\n",
    "\n",
    "# 3. Log transformation for the loan amount and loan payments variables - due to their skewness distributions\n",
    "X[\"monto\"] = np.log(X[\"monto\"])\n",
    "X[\"cuota\"] = np.log(X[\"cuota\"])\n",
    "\n",
    "# 4. train & test split\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets add some new features related to the analyst and office proportons of risk credits in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. creation or transformation of variables, avoiding data leakage\n",
    "\n",
    "# initialize transformers\n",
    "analyst_transformer = AnalystRiskRateTransformer()\n",
    "office_transformer = OfficeRiskRateTransformer()\n",
    "\n",
    "# train and transform training set\n",
    "\n",
    "# new analyst tags\n",
    "X_train_multi = analyst_transformer.fit(X_train_multi).transform(X_train_multi, drop_risk=False)\n",
    "X_test_multi = analyst_transformer.transform(X_test_multi, drop_risk=False)\n",
    "\n",
    "#new office tags\n",
    "X_train_multi = office_transformer.fit(X_train_multi).transform(X_train_multi, drop_risk=True)\n",
    "X_test_multi = office_transformer.transform(X_test_multi, drop_risk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. One hot encoding and scaling of variables\n",
    "\n",
    "# One hot encoding of categorical and object variables -- Min max scaler of numerical variables\n",
    "categorical_features = X_train_multi.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "numerical_features = X_train_multi.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\"]).columns.tolist()\n",
    "\n",
    "# The one hot encoding drop the first level to avoid multicolineality\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", MinMaxScaler(), numerical_features),  # MinMax\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False), categorical_features),  # One-Hot Encoding\n",
    "    ],\n",
    "    remainder=\"passthrough\" \n",
    ")\n",
    "\n",
    "# Fit the preprocessor only with the training set to avoid data leakage.\n",
    "X_train_tranformed = preprocessor.fit_transform(X_train_multi)\n",
    "X_test_transformed = preprocessor.transform(X_test_multi)\n",
    "\n",
    "# Obtain feature names after transformation\n",
    "encoded_features = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_features)\n",
    "column_names = numerical_features + encoded_features.tolist()\n",
    "\n",
    "# Convert the results back to DataFrame\n",
    "X_train_multi = pd.DataFrame(X_train_tranformed, columns=column_names, index=X_train_multi.index)\n",
    "X_test_multi = pd.DataFrame(X_test_transformed, columns=column_names, index=X_test_multi.index)\n",
    "\n",
    "del X_train_tranformed, X_test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2328 entries, 0 to 2327\n",
      "Columns: 119 entries, monto to estado_normal\n",
      "dtypes: float64(119)\n",
      "memory usage: 2.1 MB\n"
     ]
    }
   ],
   "source": [
    "X_train_multi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 582 entries, 0 to 581\n",
      "Columns: 119 entries, monto to estado_normal\n",
      "dtypes: float64(119)\n",
      "memory usage: 541.2 KB\n"
     ]
    }
   ],
   "source": [
    "X_test_multi.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete variables and objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling phase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
